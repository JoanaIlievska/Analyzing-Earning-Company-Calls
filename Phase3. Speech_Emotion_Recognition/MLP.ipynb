{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1sFgbgjeT-X655A0DULeoLhbpTZTAl6JW","authorship_tag":"ABX9TyMhhx2ALQ4I+DtQdNY5Amml"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lh5n6K9lJqlc","executionInfo":{"status":"ok","timestamp":1677540798516,"user_tz":-60,"elapsed":22859,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}},"outputId":"bc0120b1-4abc-4371-a596-0400f58966fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: librosa in /usr/local/lib/python3.8/dist-packages (0.8.1)\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.8/dist-packages (0.12.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.22.4)\n","Collecting sklearn\n","  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pyaudio\n","  Downloading PyAudio-0.2.13.tar.gz (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (23.0)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.0.2)\n","Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (3.0.0)\n","Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.56.4)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.7.3)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from librosa) (0.4.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.8/dist-packages (from librosa) (1.2.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile) (2.21)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (6.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->librosa) (0.39.1)\n","Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from pooch>=1.0->librosa) (2.25.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (4.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.43.0->librosa) (3.14.0)\n","Building wheels for collected packages: sklearn, pyaudio\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=448fdf646a1247667b3f303ffd63d4e7a197f0f139c49824c5773d8d4dae917b\n","  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pyaudio \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully built sklearn\n","Failed to build pyaudio\n","\u001b[31mERROR: Could not build wheels for pyaudio, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install librosa soundfile numpy sklearn pyaudio"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8mPfV_y-OGTg","executionInfo":{"status":"ok","timestamp":1677540816582,"user_tz":-60,"elapsed":14393,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}},"outputId":"930d853e-a1b3-47e9-aff9-b682e0ec6dfc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import librosa\n","import soundfile\n","import os, glob, pickle\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"B80lUeeoJrL7","executionInfo":{"status":"ok","timestamp":1677547430980,"user_tz":-60,"elapsed":789,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}}},"execution_count":127,"outputs":[]},{"cell_type":"code","source":["#DataFlair - Extract features (mfcc, chroma, mel) from a sound file\n","def extract_feature(file_name, mfcc, chroma, mel):\n","    with soundfile.SoundFile(file_name) as sound_file:\n","        X = sound_file.read(dtype=\"float32\")\n","        sample_rate=sound_file.samplerate\n","        if chroma:\n","            stft=np.abs(librosa.stft(X))\n","        result=np.array([])\n","        if mfcc:\n","            mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n","            result=np.hstack((result, mfccs))\n","        if chroma:\n","            chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n","            result=np.hstack((result, chroma))\n","        if mel:\n","            mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n","            result=np.hstack((result, mel))\n","        return result"],"metadata":{"id":"cn0BICpeJvvb","executionInfo":{"status":"ok","timestamp":1677547431749,"user_tz":-60,"elapsed":7,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}}},"execution_count":128,"outputs":[]},{"cell_type":"code","source":["emotions={\n","  '01':'neutral',\n","  '02':'calm',\n","  '03':'happy',\n","  '04':'sad',\n","  '05':'angry',\n","  '06':'fearful',\n","  '07':'disgust',\n","  '08':'surprised',\n","  'neutral': 'Neutral',\n","  'calm': 'Calm',\n","  'happy': 'Happy',\n","  'sad': 'Sad',\n","  'angry': 'Angry',\n","  'fearful': 'Fearful',\n","  'disgust': 'Disgust',\n","  'surprised': 'Surprised'\n","}\n","#DataFlair - Emotions to observe\n","observed_emotions=['happy', 'sad','calm','angry']"],"metadata":{"id":"-BwvapiUJ3Sb","executionInfo":{"status":"ok","timestamp":1677547433664,"user_tz":-60,"elapsed":5,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}}},"execution_count":129,"outputs":[]},{"cell_type":"code","source":["#DataFlair - Load the data and extract features for each sound file\n","def load_data(test_size=0.2):\n","    x,y=[],[]\n","    for file in glob.glob(\"/content/drive/MyDrive/DataScienceAdvanced/Phase3:SER/TrainData/Actor_*/*.wav\"):\n","        file_name=os.path.basename(file)\n","        emotion=emotions[file_name.split(\"-\")[2]]\n","        if emotion not in observed_emotions:\n","            continue\n","        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n","        x.append(feature)\n","        y.append(emotion)\n","    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)"],"metadata":{"id":"siTGccLDZraM","executionInfo":{"status":"ok","timestamp":1677547436004,"user_tz":-60,"elapsed":7,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}}},"execution_count":130,"outputs":[]},{"cell_type":"code","source":["#DataFlair - Split the dataset\n","x_train,x_test,y_train,y_test=load_data(test_size=0.25)"],"metadata":{"id":"tsZei8k4KKt9","executionInfo":{"status":"ok","timestamp":1677547465179,"user_tz":-60,"elapsed":28520,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}}},"execution_count":131,"outputs":[]},{"cell_type":"code","source":["#DataFlair - Get the shape of the training and testing datasets\n","print((x_train.shape[0], x_test.shape[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHjMuvg4KLoP","executionInfo":{"status":"ok","timestamp":1677547479370,"user_tz":-60,"elapsed":637,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}},"outputId":"ec319693-32c9-4f11-f8de-36051df82524"},"execution_count":132,"outputs":[{"output_type":"stream","name":"stdout","text":["(576, 192)\n"]}]},{"cell_type":"code","source":["#DataFlair - Initialize the Multi Layer Perceptron Classifier\n","model=MLPClassifier(alpha=0.01, batch_size=512, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=600)"],"metadata":{"id":"2iHe6gVWKPEE","executionInfo":{"status":"ok","timestamp":1677547481718,"user_tz":-60,"elapsed":5,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}}},"execution_count":133,"outputs":[]},{"cell_type":"code","source":["#DataFlair - Train the model\n","\n","\n","model.fit(x_train,y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F_6Ppc8AKSFm","executionInfo":{"status":"ok","timestamp":1677547488524,"user_tz":-60,"elapsed":2135,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}},"outputId":"6109166d-5427-42d3-e1e6-01923a07ddb6"},"execution_count":134,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(alpha=0.01, batch_size=512, hidden_layer_sizes=(300,),\n","              learning_rate='adaptive', max_iter=600)"]},"metadata":{},"execution_count":134}]},{"cell_type":"code","source":["#DataFlair - Predict for the test set\n","y_pred=model.predict(x_test)\n","y_pred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qjCwQibHKUiX","executionInfo":{"status":"ok","timestamp":1677547490945,"user_tz":-60,"elapsed":481,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}},"outputId":"d32179b8-2e65-4c9a-8097-eb7be0948043"},"execution_count":135,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['calm', 'sad', 'angry', 'angry', 'angry', 'happy', 'calm', 'calm',\n","       'calm', 'calm', 'calm', 'calm', 'happy', 'happy', 'calm', 'calm',\n","       'angry', 'calm', 'calm', 'calm', 'sad', 'happy', 'calm', 'happy',\n","       'happy', 'calm', 'calm', 'calm', 'happy', 'angry', 'angry', 'sad',\n","       'angry', 'calm', 'calm', 'angry', 'calm', 'calm', 'sad', 'calm',\n","       'sad', 'happy', 'calm', 'happy', 'happy', 'happy', 'angry',\n","       'happy', 'calm', 'calm', 'happy', 'calm', 'happy', 'calm', 'happy',\n","       'calm', 'calm', 'calm', 'angry', 'calm', 'happy', 'angry', 'calm',\n","       'calm', 'calm', 'sad', 'angry', 'angry', 'calm', 'calm', 'happy',\n","       'angry', 'calm', 'calm', 'calm', 'angry', 'calm', 'happy', 'angry',\n","       'happy', 'happy', 'sad', 'happy', 'angry', 'angry', 'happy',\n","       'calm', 'happy', 'angry', 'sad', 'happy', 'calm', 'happy', 'calm',\n","       'angry', 'happy', 'happy', 'calm', 'calm', 'calm', 'happy', 'calm',\n","       'sad', 'calm', 'happy', 'sad', 'angry', 'angry', 'angry', 'calm',\n","       'angry', 'sad', 'happy', 'calm', 'angry', 'happy', 'angry', 'calm',\n","       'angry', 'calm', 'happy', 'calm', 'happy', 'calm', 'calm', 'angry',\n","       'calm', 'happy', 'calm', 'sad', 'sad', 'calm', 'calm', 'sad',\n","       'calm', 'happy', 'happy', 'happy', 'happy', 'calm', 'calm', 'calm',\n","       'happy', 'happy', 'angry', 'calm', 'calm', 'sad', 'happy', 'happy',\n","       'angry', 'angry', 'calm', 'angry', 'angry', 'calm', 'calm',\n","       'happy', 'angry', 'happy', 'happy', 'happy', 'calm', 'calm',\n","       'calm', 'angry', 'calm', 'angry', 'angry', 'angry', 'angry', 'sad',\n","       'sad', 'calm', 'calm', 'happy', 'angry', 'calm', 'calm', 'calm',\n","       'calm', 'angry', 'calm', 'sad', 'sad', 'sad', 'angry', 'angry',\n","       'calm', 'angry', 'sad', 'calm'], dtype='<U5')"]},"metadata":{},"execution_count":135}]},{"cell_type":"code","source":["#DataFlair - Calculate the accuracy of our model\n","accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n","\n","#DataFlair - Print the accuracy\n","print(\"Accuracy: {:.2f}%\".format(accuracy*100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CE8tdVZaKYZk","executionInfo":{"status":"ok","timestamp":1677547497574,"user_tz":-60,"elapsed":1245,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}},"outputId":"845aec52-974a-487c-9610-529c4760c769"},"execution_count":136,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 70.31%\n"]}]},{"cell_type":"code","source":["# Define function to extract audio features\n","def extract_feature(audio_data, sample_rate, mfcc=True, chroma=True, mel=True):\n","    feature_list = []\n","    if chroma:\n","        stft = np.abs(librosa.stft(audio_data))\n","        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n","        feature_list.append(chroma)\n","    if mfcc:\n","        mfccs = np.mean(librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40).T,axis=0)\n","        feature_list.append(mfccs)\n","    if mel:\n","        mel = np.mean(librosa.feature.melspectrogram(audio_data, sr=sample_rate).T,axis=0)\n","        feature_list.append(mel)\n","    return np.concatenate(feature_list)\n","\n","# Load audio file\n","audio_file = '/content/30 Second Coca Cola Commercial.wav'\n","y, sr = librosa.load(audio_file, mono=False)\n","\n","# Convert stereo audio to monophonic audio\n","y_mono = librosa.to_mono(y)\n","\n","# Split audio into 3-second chunks\n","chunk_length = sr * 3# 3 seconds\n","chunks = librosa.util.frame(y_mono, frame_length=chunk_length, hop_length=chunk_length)\n","\n","# Extract features from each chunk\n","features = []\n","for chunk in chunks.T:\n","    feature = extract_feature(chunk, sample_rate=sr, mfcc=True, chroma=True, mel=True)\n","    features.append(feature)\n","\n","# Reshape audio features\n","features = np.array(features)\n","n_samples = features.shape[0]\n","features = features.reshape(n_samples, -1) # reshape to have 180 features\n","\n","# Predict emotion label\n","predicted_emotions = model.predict(features)\n","\n","# Map predicted emotion codes to emotion labels\n","predicted_emotions = [emotions[prediction] for prediction in predicted_emotions]\n","\n","# Print predicted emotion labels for each chunk\n","for i, emotion in enumerate(predicted_emotions):\n","    print(f\"Chunk {i+1}: {emotion}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vrSksdeEzi-j","executionInfo":{"status":"ok","timestamp":1677547519223,"user_tz":-60,"elapsed":3252,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}},"outputId":"a1aa3434-ab1a-4220-c5be-37921f2654d4"},"execution_count":139,"outputs":[{"output_type":"stream","name":"stdout","text":["Chunk 1: Angry\n","Chunk 2: Angry\n","Chunk 3: Angry\n","Chunk 4: Angry\n","Chunk 5: Angry\n","Chunk 6: Angry\n","Chunk 7: Angry\n","Chunk 8: Angry\n","Chunk 9: Angry\n"]}]},{"cell_type":"code","source":["from collections import Counter\n","\n","# Predict emotion labels for each chunk\n","# Map predicted emotion codes to emotion labels\n","predicted_emotions = [emotions[prediction.lower()] for prediction in predicted_emotions]\n","\n","# Count occurrences of each emotion\n","emotion_counts = Counter(predicted_emotions)\n","\n","# Get emotion with the highest count\n","majority_emotion = emotion_counts.most_common(1)[0][0]\n","\n","print('Majority emotion:', majority_emotion)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tQdBot6t1snS","executionInfo":{"status":"ok","timestamp":1677547417161,"user_tz":-60,"elapsed":468,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}},"outputId":"38a63d50-8892-4a5d-b5b9-90a86c35bc75"},"execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["Majority emotion: Angry\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bxukj_g02JfJ"},"execution_count":null,"outputs":[]}]}