{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"16Y4wnkNWwepus_12eqpQofznEjCPPWBA","authorship_tag":"ABX9TyMXYypKrsh42PJRErqxQa8S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import chardet\n","import re\n","\n","def wer(reference, hypothesis):\n","    \"\"\"\n","    Computes the Word Error Rate (WER) between two text files, ignoring case and punctuation.\n","    \n","    Args:\n","    - reference (str): path to the reference text file\n","    - hypothesis (str): path to the hypothesis text file\n","    \n","    Returns:\n","    - wer (float): the WER between the reference and hypothesis files\n","    \"\"\"\n","    # Detect the encoding of the input files\n","    with open(reference, 'rb') as ref_file:\n","        ref_data = ref_file.read()\n","        ref_encoding = chardet.detect(ref_data)['encoding']\n","    with open(hypothesis, 'rb') as hyp_file:\n","        hyp_data = hyp_file.read()\n","        hyp_encoding = chardet.detect(hyp_data)['encoding']\n","    \n","    # Read the files and split them into lists of words, ignoring case and punctuation\n","    with open(reference, \"r\", encoding=ref_encoding) as ref_file:\n","        ref_words = re.findall(r'\\w+', ref_file.read().lower())\n","    with open(hypothesis, \"r\", encoding=hyp_encoding) as hyp_file:\n","        hyp_words = re.findall(r'\\w+', hyp_file.read().lower())\n","    \n","    # Initialize the matrix to store the edit distances between all pairs of words\n","    # We add 1 to the dimensions because the matrix is 1-indexed\n","    D = np.zeros((len(ref_words) + 1, len(hyp_words) + 1))\n","    for i in range(len(ref_words) + 1):\n","        D[i, 0] = i\n","    for j in range(len(hyp_words) + 1):\n","        D[0, j] = j\n","    \n","    # Compute the edit distances between all pairs of words\n","    for i in range(1, len(ref_words) + 1):\n","        for j in range(1, len(hyp_words) + 1):\n","            if ref_words[i-1] == hyp_words[j-1]:\n","                D[i, j] = D[i-1, j-1]\n","            else:\n","                D[i, j] = min(D[i-1, j], D[i, j-1], D[i-1, j-1]) + 1\n","    \n","    # Compute the WER as the normalized edit distance between the reference and hypothesis\n","    wer = D[len(ref_words), len(hyp_words)] / len(ref_words)\n","    \n","    return wer\n"],"metadata":{"id":"z_vQeVurUr5m","executionInfo":{"status":"ok","timestamp":1677011298026,"user_tz":-60,"elapsed":903,"user":{"displayName":"Joana Ilievska","userId":"01886870701199404073"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["reference_file = \"\"\n","hypothesis_file = \"\"\n","wer_score = wer(reference_file, hypothesis_file)\n","print(\"WER:\", wer_score)\n"],"metadata":{"id":"iR6EoUuwTxzz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xmU9FZZSaQI4"},"execution_count":null,"outputs":[]}]}